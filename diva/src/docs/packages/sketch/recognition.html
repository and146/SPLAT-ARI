<!-- Copyright (c) 1998-2001 The Regents of the University of California. -->
<!-- All rights reserved. See the file COPYRIGHT for details.             -->
<HTML>

<HEAD>
<LINK href="../../diva.css" rel="stylesheet" type="text/css">

	<TITLE>Recognition package design document</TITLE>
</HEAD>

<BODY BGCOLOR="#FFFFFF">

<!--navbar-->


<!--/navbar-->
<H1>Diva Sketch Recognition</H1>
<H2>Introduction</H2>
<P>This package provides a recognition framework in which modular recognition elements can be combined in useful<BR>
ways. The class RecognitionEngine abstracts the package by providing a facade over the entire recognition process.
<H2>Architecture Overview</H2>
<CENTER>
<P>
<TABLE BORDER="1" WIDTH="34%">
	<TR>
		<TD WIDTH="98%">
			<P ALIGN="CENTER"><IMG SRC="images/recognitionEngine.gif" WIDTH="299" HEIGHT="382" ALIGN="BOTTOM" BORDER="0">
		</TD>
		<TD WIDTH="2%"><IMG SRC="images/voting.gif" WIDTH="402" HEIGHT="395" ALIGN="BOTTOM" BORDER="0"></TD>
	</TR>
	<TR>
		<TD WIDTH="98%">
			<P ALIGN="CENTER">Recognition engine UML diagram
		</TD>
		<TD WIDTH="2%">
			<P ALIGN="CENTER">Recognition engine using voting recognizers
		</TD>
	</TR>
</TABLE>
</P>
</CENTER>
<P>The architecture of the recognition system is composed of component-like building blocks. There are three major
stages in the recognition process: gesture assembly, gesture recognition and reclassification.</P>
<H3><B>Gesture Assembly</B></H3>
<P>Mouse events are taken as input into the system and assembled into <I>gestures</I> consisting of one or more
<I>pen strokes</I>. This stage is embodied by the <B>GestureAssembler</B> class.</P>
<P>More specifically, GestureAssembler provides support for multi-stroke gestures as follows. When GestureAssembler
receives a mouse pressed event, it starts collecting points until a mouse released event happens. The points collected
are packaged into a PenStroke object. If the next mouse pressed event is received within a certain period of time
after the previous stroke, then this becomes the second stroke in a gesture, and so on. If no mouse pressed event
is received within a certain period of time after the previous stroke, then the gesture is completed and a gesture
event and a change event is emitted. The default between-stroke waiting period is set to 500 ms.</P>
<H3>Gesture Recognition</H3>
<P>During this stage of the recognition process, gestures (either fully or partially completed) are analyzed and
recognized, producing classifications. This process can be done either with or without knowledge of the application.
The stage is embodied by the GestureRecognizer class.</P>
<P>ClassifyingRecognizer is a type of GestureRecognizer. It uses a generic pattern classifier to recognize gestures
and generate [type, confidence] pairs for each gesture type that it has been trained with. In the current implementation,
there are two steps that are performed on a gesture before it is classified. The first step is preprocessing to
remove noise and reduce gesture complexity (an &quot;approximation by line segments&quot; [1] technique is used
here). The second step is feature extraction which generates a set of features from the gesture. This set of features
is input to the classifier to see how closely the set of features resembles a class. Feature extractors are one
way that the recognition process can be customized or enhanced by the user. We provide a number of general feature
extractors in the package, but these can be replaced or extended by the user to tune the recognition. The default
feature extractors used in ClassifyingRecognizer are:</P>

<OL>
	<LI>cosine of the initial angle
	<LI>sine of the initial angle
	<LI>angle between the diagonal and the base of a gesture's bounding box
	<LI>distance between the start and the end point
	<LI>cosine of the angle between the first and last point
	<LI>sine of the angle between the first and last point
	<LI>sum of angles along a gesture's path
	<LI>sum of the absolute values of angles along a gesture's path
	<LI>sum of the squares of angles along a gesture's path
	<LI>ratio of a gesture's convex hull area and its bounding box area, (convex_hull/bbox).
	<LI>ratio of the width and height of a gesture (width/height)
	<LI>number of corners in a gesture
</OL>

<P>The first 9 features are taken from Rubine's paper. [2]</P>
<P>Finally, VotingGestureRecognizer is a composite GestureRecognizer which allows multiple gesture recognizers
to &quot;vote&quot; on the classification of a gesture. This way trained reusable gesture recognizers such as ClassifyingRecognizer
can be used alongside specialized or application-specific gesture recognizers such as diva.sketch.toolbox.ScribbleRecognizer.</P>
<H3>Reclassification</H3>
<P>Classifications given by the second stage of the process are reclassified using contextual information. The
result is output to the application. This stage is embodied by the <B>ClassificationRecognizer</B> interface, which
is analagous to <B>GestureRecognizer</B> except that it processes <B>ClassificationEvents</B> instead of <B>GestureEvents</B>.
The trait of a ClassificationRecognizer is that it performs recognition based on the result of other recognizers.
Among other things, it can be used to implement a high-level recognizer as described in [3, 4].</P>
<H3>References</H3>
<P>[1] &quot;An On-line Character Recognition Aimed at a Substitution for a Billing Machine Keyboard&quot; Hanaki,
Temma, Yoshida, Pattern Recognition, Vol.8, pp63-71, 1976.</P>
<P>[2] &quot;Specifying Gestures by Example&quot; Dean Rubine, Computer Graphics, Volume25, Number4, July 1991.</P>
<P>[3] &quot;Pen-based Graphical User Interfaces for Conceptual Sketching&quot; Rui Zhao, International Conference
Proceedings, 1994.</P>
<P>[4] &quot;Recognizing and Interpreting Diagrams in Design&quot; Gross, M.D. In T. Catarci. M. Costabile, S.
Levialdi, G. Santucci eds., Advanced Visual Interfaces '94 (AVI '94), ACM Press.</P>
<P><!--footer-->


<!--/footer-->

</BODY>

</HTML>




